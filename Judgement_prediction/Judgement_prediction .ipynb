{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-X2mk2pifsnF"
   },
   "outputs": [],
   "source": [
    "!pip install transformers # installing tranformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxPFOWypS-CT"
   },
   "outputs": [],
   "source": [
    "# importing modules and packages\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import json\n",
    "import progressbar\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjReumy4T6Hf"
   },
   "outputs": [],
   "source": [
    "# mounting drive in colab\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHgUGo7pCmrz"
   },
   "outputs": [],
   "source": [
    "file=open(\"/content/judgement_prediction_test.txt\",'r') # path of the text file with test filenames\n",
    "file_name=file.read()\n",
    "list_of_file_names=file_name.split('\\n') # list containing the names of the test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SbyCttCMTjm"
   },
   "outputs": [],
   "source": [
    "tag_doc=open(\"/content/tag.txt\",'r') # path of the text file with filenames and its respective tags as dictionary\n",
    "tag_doc_text=tag_doc.read()\n",
    "list_tags_as_string=[]\n",
    "tag_doc_text=eval(tag_doc_text) #converting the string to dictionary\n",
    "for fl_name in tag_doc_text:\n",
    "  str=''\n",
    "  for tag in tag_doc_text[fl_name]:\n",
    "    str+=tag+' '\n",
    "  list_tags_as_string.append(str) # appending the tags as space separated string in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcIjjZPovHP1"
   },
   "outputs": [],
   "source": [
    "# Loading the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "model = AutoModel.from_pretrained(\"law-ai/InLegalBERT\")\n",
    "\n",
    "\n",
    "# Load the Longformer model and tokenizer\n",
    "'''model_name = 'allenai/longformer-base-4096'\n",
    "tokenizer = LongformerTokenizer.from_pretrained(model_name)\n",
    "model = LongformerModel.from_pretrained(model_name)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "317AIGtjS6Al"
   },
   "outputs": [],
   "source": [
    "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")\n",
    "#model = AutoModel.from_pretrained(\"nlpaueb/legal-bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQYDS_gxUJWC"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('/content/drive/MyDrive/ILDC_single.csv') # path of the csv file containg the train and test set\n",
    "train_set= df.query(\" split=='train' \") #separting the train set\n",
    "test_set= df.query(\" split=='test' \") # separating the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78h-qpIjTKXK"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def generate_np_files_for_training(text, tokenizer): # function to return the embedding of a text using above tokenizer and model\n",
    "  all_input_ids, all_att_masks, all_labels = [], [], []\n",
    "  toks = tokenizer.tokenize(text)\n",
    "  if(len(toks) > 10000): # if the token size is more than 10000, drop the first 10000 tokens\n",
    "      toks = toks[len(toks)-10000:]\n",
    "  tokens = ['[CLS]'] + toks[-510:] + ['[SEP]'] # model uses 512 tokens\n",
    "  input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "  input_tensor = torch.tensor(input_ids)\n",
    "  # Get the model's output\n",
    "  with torch.no_grad():\n",
    "    output = model(input_tensor.unsqueeze(0))\n",
    "  embedding = output.last_hidden_state[:, 0, :]\n",
    "  embedded=embedding.tolist()\n",
    "  return embedded[0] #returning the embedding as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VAbJd5doEFWv"
   },
   "outputs": [],
   "source": [
    "def embed(dataf): #function to return embedding of all text's in a dataframe\n",
    "  embedded_list=[]\n",
    "  for i in progressbar.progressbar(range(len(dataf['text']))):\n",
    "    text = dataf['text'].iloc[i]\n",
    "    k=generate_np_files_for_training(text, tokenizer) # calling the above method and passing the text and loaded tokenizer\n",
    "    embedded_list.append(k)\n",
    "  return embedded_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SPD-qrlRhJsC"
   },
   "outputs": [],
   "source": [
    "train_embedding=embed(train_set) #embedding of the whole train set\n",
    "X=np.array(train_embedding)\n",
    "np.save(\"/content/drive/MyDrive/experiment/inlegaltraining.npy\", X) #saving the embedding of the train set as a binary file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDvd9pcmW7yW"
   },
   "outputs": [],
   "source": [
    "test_filename=test_set[\"name\"].tolist() # names of all testfile\n",
    "lst1,lst2,lst3=[],[],[]\n",
    "for i in list_of_file_names:\n",
    "  set,set1=[],[]\n",
    "  if i in test_filename:\n",
    "    set=generate_np_files_for_training(test_set.loc[5082+test_filename.index(i)].at[\"text\"],tokenizer) # embedding of the text of the test file without appended tag\n",
    "    set1=generate_np_files_for_training(test_set.loc[5082+test_filename.index(i)].at[\"text\"]+list_tags_as_string[list_of_file_names.index(i)],tokenizer) #embedding of the text with appended tag\n",
    "    lst2.append(int(test_set.loc[5082+test_filename.index(i)].at[\"label\"])) # extracting the original label of the test files\n",
    "    lst1.append(set)\n",
    "    lst3.append(set1)\n",
    "embedding_without_appended_tag=np.array(lst1)\n",
    "embedding_appended_text=np.array(lst3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sw2fiszkfjqg"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_embedding=np.load(\"/content/drive/MyDrive/experiment/inlegaltraining.npy\",allow_pickle=True) #loading the binary file with embedding of the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8Qjew1zhQxj"
   },
   "outputs": [],
   "source": [
    "'''def Listing(a):\n",
    "  l=[]\n",
    "  for i in a:\n",
    "    x=i.tolist()\n",
    "    l.append(x)\n",
    "  return l'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwQtx7PshSFr"
   },
   "outputs": [],
   "source": [
    "'''def padding_zeros(a):\n",
    "  for i in a:\n",
    "    i.extend([0]*(768))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgb2xeABhwz4"
   },
   "outputs": [],
   "source": [
    "'''train=Listing(tr)\n",
    "padding_zeros(train)\n",
    "trn=np.array(train)\n",
    "tst=np.array(lst1)\n",
    "print(len(tst[0]))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgYEHfz4jXUD"
   },
   "outputs": [],
   "source": [
    "train_label = train_set['label'].to_numpy().astype('int') # labels of the train set\n",
    "test_label=np.array(lst2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0JAMoEBk1gI"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # importing Logistic Regression from sklearn\n",
    "from sklearn.metrics import accuracy_score # importing accuracy_score\n",
    "clf = LogisticRegression() # loading LR\n",
    "clf.fit(train_embedding,train_label) # fitting LR with our train set embedding and labels\n",
    "test_predicted_label_w_tag=clf.predict(embedding_appended_text) # predicted label of test set with appended tag at the end\n",
    "test_predicted_label_wo_tag=clf.predict(embedding_without_appended_tag) #predicted labe of test set without appended tag\n",
    "acc1=accuracy_score(test_label,test_predicted_label_w_tag) # accuracy calculation using original and predicted label\n",
    "acc=accuracy_score(test_label,test_predicted_label_wo_tag)\n",
    "print(\"Accuracy without appending tag:\",acc)\n",
    "print(\"Accuracy with appended tag:\",acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ev-kY6cx0Ks"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf=SVC(kernel=\"linear\")\n",
    "clf.fit(tr,trlabel)\n",
    "t_label=clf.predict(tst)\n",
    "t1_label=clf.predict(tst2)\n",
    "acc=accuracy_score(tst1,t_label)\n",
    "acc1=accuracy_score(tst1,t1_label)\n",
    "print(acc,acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8xAZ_0lxURB"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC # importing Lofistic Regression from sklearn\n",
    "from sklearn.metrics import accuracy_score # importing accuracy_score\n",
    "clf = SVC(kernel=\"linear\") # loading Linear SVC\n",
    "clf.fit(train_embedding,train_label) # fitting LR with our train set embedding and labels\n",
    "test_predicted_label_w_tag=clf.predict(embedding_appended_text) # predicted label of test set with appended tag at the end\n",
    "test_predicted_label_wo_tag=clf.predict(embedding_without_appended_tag) #predicted labe of test set without appended tag\n",
    "acc1=accuracy_score(test_label,test_predicted_label_w_tag) # accuracy calculation using original and predicted label\n",
    "acc=accuracy_score(test_label,test_predicted_label_wo_tag)\n",
    "print(\"Accuracy without appending tag:\",acc)\n",
    "print(\"Accuracy with appended tag:\",acc1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
